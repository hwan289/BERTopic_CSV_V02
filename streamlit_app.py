import os
import sys
import time
import streamlit as st
import pandas as pd
import io
import numpy as np

# --- CONFIGURATION: Force Single Threading ---
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# -----------------------------------------------------------------------------
# Page Configuration
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="BERTopic Explorer",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# -----------------------------------------------------------------------------
# Check Environment
# -----------------------------------------------------------------------------
if sys.platform == "darwin":
    if os.environ.get('OBJC_DISABLE_INITIALIZE_FORK_SAFETY') != 'YES':
        st.error("""
        ğŸ›‘ **Critical Error: App launched incorrectly (Local Mac)**
        Please run using the `run_app.sh` script.
        """)
        st.stop()

# -----------------------------------------------------------------------------
# Language & Translations
# -----------------------------------------------------------------------------
if 'lang' not in st.session_state:
    st.session_state['lang'] = 'en'

def toggle_language():
    st.session_state['lang'] = 'zh' if st.session_state['lang'] == 'en' else 'en'

TRANS = {
    'title': { 'en': "ğŸ§  BERTopic Interactive Explorer", 'zh': "ğŸ§  BERTopic äº¤äº’å¼æ¢ç´¢å™¨" },
    'desc': {'en': "Advanced Topic Modeling with BERTopic.", 'zh': "BERTopic é«˜çº§ä¸»é¢˜å»ºæ¨¡ã€‚"},
    'sidebar_config': {'en': "Configuration", 'zh': "é…ç½®"},
    'remove_stopwords': {'en': "Remove Stopwords (English)", 'zh': "ç§»é™¤åœç”¨è¯ (è‹±æ–‡)"},
    'lemmatize': {'en': "Combine Variations (Lemmatize)", 'zh': "åˆå¹¶è¯å½¢å˜ä½“ (Lemmatize)"},
    'lemmatize_help': {'en': "Converts words to base form (e.g., 'students' -> 'student'). Slower but cleaner.", 'zh': "å°†å•è¯è½¬æ¢ä¸ºåŸºæœ¬å½¢å¼ï¼ˆä¾‹å¦‚ 'students' -> 'student'ï¼‰ã€‚é€Ÿåº¦è¾ƒæ…¢ä½†ç»“æœæ›´æ•´æ´ã€‚"},
    'data_loading': {'en': "Data Loading", 'zh': "æ•°æ®åŠ è½½"},
    'upload_csv': {'en': "Upload CSV", 'zh': "ä¸Šä¼  CSV"},
    'train_btn': {'en': "ğŸš€ Train BERTopic Model", 'zh': "ğŸš€ è®­ç»ƒ BERTopic æ¨¡å‹"},
    'status_start': {'en': "Starting Process...", 'zh': "æ­£åœ¨å¯åŠ¨æµç¨‹..."},
    'step_1': {'en': "âš™ï¸ [1/3] Configuring & Importing...", 'zh': "âš™ï¸ [1/3] é…ç½®ä¸å¯¼å…¥..."},
    'step_2': {'en': "ğŸƒ [2/3] Processing Topics...", 'zh': "ğŸƒ [2/3] å¤„ç†ä¸»é¢˜ä¸­..."},
    'step_3': {'en': "âœ… [3/3] Done!", 'zh': "âœ… [3/3] å®Œæˆ!"},
    'train_complete': {'en': "Complete! Time: {:.2f}s", 'zh': "å®Œæˆ! è€—æ—¶: {:.2f} ç§’"},
    'results_header': {'en': "Results Analysis", 'zh': "ç»“æœåˆ†æ"},
    'upload_prompt': {'en': "Please upload a CSV file to begin.", 'zh': "è¯·ä¸Šä¼  CSV æ–‡ä»¶ä»¥å¼€å§‹ã€‚"},
    'no_topics_warning': {
        'en': "âš ï¸ No topics were found! Everything was classified as outliers (-1). Try decreasing 'Min Topic Size' or adding more data.",
        'zh': "âš ï¸ æœªå‘ç°ä»»ä½•ä¸»é¢˜ï¼æ‰€æœ‰å†…å®¹éƒ½è¢«å½’ç±»ä¸ºç¦»ç¾¤å€¼ (-1)ã€‚è¯·å°è¯•å‡å°â€œæœ€å°ä¸»é¢˜å¤§å°â€æˆ–æ·»åŠ æ›´å¤šæ•°æ®ã€‚"
    },
    'viz_error': {'en': "Visualization not available: {}", 'zh': "æ— æ³•ç”Ÿæˆå¯è§†åŒ–: {}"},
    'help_info_title': {'en': "â„¹ï¸ How to interpret this table", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»æ­¤è¡¨"},
    'help_info_text': {
        'en': "**Topic:** The ID of the topic. -1 refers to 'outliers' (noise).\n**Count:** Documents in this topic.\n**Name:** Keywords representing the topic.",
        'zh': "**Topic:** ä¸»é¢˜ IDã€‚-1 ä»£è¡¨â€œç¦»ç¾¤å€¼â€ï¼ˆå™ªéŸ³ï¼‰ã€‚\n**Count:** æ–‡æ¡£æ•°é‡ã€‚\n**Name:** ä»£è¡¨è¯¥ä¸»é¢˜çš„å…³é”®è¯ã€‚"
    },
    'help_dist_title': {'en': "â„¹ï¸ How to interpret the Distance Map", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»è·ç¦»å›¾"},
    'help_dist_text': {
        'en': "**Circles:** Topics.\n**Distance:** Closer circles = Similar meanings.",
        'zh': "**åœ†åœˆ:** ä¸»é¢˜ã€‚\n**è·ç¦»:** åœ†åœˆè¶Šè¿‘ = å«ä¹‰è¶Šç›¸ä¼¼ã€‚"
    },
    'help_bar_title': {'en': "â„¹ï¸ How to interpret the Bar Chart", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»æ¡å½¢å›¾"},
    'help_bar_text': {
        'en': "Shows distinct keywords for each topic based on c-TF-IDF score.",
        'zh': "åŸºäº c-TF-IDF åˆ†æ•°æ˜¾ç¤ºæ¯ä¸ªä¸»é¢˜çš„ç‹¬ç‰¹å…³é”®è¯ã€‚"
    },
    'help_heat_title': {'en': "â„¹ï¸ How to interpret the Similarity Heatmap", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»ç›¸ä¼¼åº¦çƒ­åŠ›å›¾"},
    'help_heat_text': {
        'en': "Shows similarity between topics. Dark blue = High similarity.",
        'zh': "æ˜¾ç¤ºä¸»é¢˜é—´çš„ç›¸ä¼¼åº¦ã€‚æ·±è“è‰² = é«˜ç›¸ä¼¼åº¦ã€‚"
    },
    # New Translations for the Representation Tab
    'rep_tab_title': {'en': "ğŸ”  Representations (KeyBERT & MMR)", 'zh': "ğŸ”  ä¸»é¢˜æè¿° (KeyBERT & MMR)"},
    'rep_help_title': {'en': "â„¹ï¸ What are these?", 'zh': "â„¹ï¸ è¿™äº›æ˜¯ä»€ä¹ˆï¼Ÿ"},
    'rep_help_text': {
        'en': "**KeyBERTInspired:** Focuses on keywords that appear frequently in the topic but less in others (Better readability).\n**MMR (Maximal Marginal Relevance):** Focuses on diversity. Reduces repetitive words like 'car', 'cars', 'vehicle'.",
        'zh': "**KeyBERTInspired:** ä¸“æ³¨äºä¸»é¢˜ä¸­é¢‘ç¹å‡ºç°ä½†åœ¨å…¶ä»–ä¸»é¢˜ä¸­è¾ƒå°‘çš„å…³é”®è¯ï¼ˆå¯è¯»æ€§æ›´å¥½ï¼‰ã€‚\n**MMR (æœ€å¤§è¾¹ç•Œç›¸å…³æ€§):** ä¸“æ³¨äºå¤šæ ·æ€§ã€‚å‡å°‘é‡å¤è¯æ±‡ï¼Œå¦‚ 'car', 'cars', 'vehicle'ã€‚"
    }
}

def t(key):
    return TRANS.get(key, {}).get(st.session_state['lang'], "Missing")

# -----------------------------------------------------------------------------
# Styling Helpers
# -----------------------------------------------------------------------------
GEMINI_BLUE = "#4285F4" 

def style_fig(fig):
    """Applies custom styling to Plotly figures."""
    if fig:
        fig.update_layout(
            title_font_color=GEMINI_BLUE,
            margin=dict(t=80), 
            hoverlabel=dict(
                bgcolor="#333333",
                font_color="#4b8bf5",
                font_family="sans-serif",
                bordercolor="#4b8bf5"
            )
        )
    return fig

# -----------------------------------------------------------------------------
# Sidebar
# -----------------------------------------------------------------------------
st.sidebar.button("ğŸŒ English / ä¸­æ–‡", on_click=toggle_language)
st.sidebar.title(t('sidebar_config'))

# Common Configurations
remove_stopwords = st.sidebar.checkbox(t('remove_stopwords'), value=True)
use_lemmatization = st.sidebar.checkbox(t('lemmatize'), value=False, help=t('lemmatize_help'))

docs = []

# --- DATA LOADING LOGIC ---
uploaded_file = st.sidebar.file_uploader(t('upload_csv'), type=["csv"])
if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file)
        text_col = st.sidebar.selectbox("Text Column", df.columns)
        df = df.dropna(subset=[text_col])
        df = df[df[text_col].astype(str).str.strip() != '']
        df = df.reset_index(drop=True)
        
        if len(df) == 0:
            st.error("Error: No valid text data found on the column.")
        else:
            docs = df[text_col].astype(str).tolist()
            st.sidebar.success(f"Loaded {len(docs)} docs")
    except Exception as e:
        st.sidebar.error(f"Error reading CSV: {e}")

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def get_lemmatizer_analyzer():
    from sklearn.feature_extraction.text import CountVectorizer
    try:
        import nltk
        from nltk.stem import WordNetLemmatizer
        try:
            nltk.data.find('corpora/wordnet')
            nltk.data.find('corpora/omw-1.4')
        except LookupError:
            with st.spinner("Downloading NLTK data..."):
                nltk.download('wordnet')
                nltk.download('omw-1.4')
    except ImportError:
        st.error("âŒ `nltk` library missing.")
        st.stop()
        
    lemmatizer = WordNetLemmatizer()
    analyzer = CountVectorizer(stop_words="english").build_analyzer()
    def lemmatized_words(doc):
        return [lemmatizer.lemmatize(w) for w in analyzer(doc)]
    return lemmatized_words

# -----------------------------------------------------------------------------
# Main App Logic
# -----------------------------------------------------------------------------
st.title(t('title'))

language = st.sidebar.selectbox("Language", ["english", "multilingual"], index=0)

st.sidebar.markdown("---")
st.sidebar.markdown("**Step 1: Discovery**")
min_topic_size = st.sidebar.number_input("Min Topic Size", min_value=2, value=5, step=1)

st.sidebar.markdown("**Step 2: Reduction**")
auto_topics = st.sidebar.checkbox("Auto Reduce Topics", value=True)

if auto_topics:
    nr_topics = "auto"
else:
    nr_topics = st.sidebar.slider("Target Max Topics", 5, 300, 20)

st.sidebar.markdown("---")
auto_adjust_params = st.sidebar.checkbox("Auto-adjust parameters", value=True)

if st.button(t('train_btn'), type="primary", disabled=(not docs)):
    start_time = time.time()
    with st.status(t('status_start'), expanded=True) as status:
        try:
            st.write(t('step_1'))
            import torch
            torch.set_num_threads(1)
            from bertopic import BERTopic
            from umap import UMAP
            from hdbscan import HDBSCAN
            from sklearn.feature_extraction.text import CountVectorizer 
            
            # ğŸ’¡ NEW IMPORTS: Import the representation models
            from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance

            # 2. Configure Vectorizer
            if use_lemmatization:
                custom_analyzer = get_lemmatizer_analyzer()
                vectorizer_model = CountVectorizer(analyzer=custom_analyzer)
            else:
                vectorizer_model = CountVectorizer(stop_words="english") if remove_stopwords else None

            # 3. Configure Sub-models & Safety
            n_samples = len(docs)
            if n_samples < 5:
                st.error("Too few documents.")
                st.stop()

            safe_min_topic_size = min_topic_size
            if min_topic_size >= n_samples:
                safe_min_topic_size = max(2, n_samples - 1)

            n_neighbors_val = 15
            n_components_val = 5
            if auto_adjust_params and n_samples < 20:
                n_neighbors_val = max(2, min(15, n_samples - 1))
                n_components_val = max(2, min(5, n_samples - 2))
            
            umap_model = UMAP(n_neighbors=n_neighbors_val, n_components=n_components_val, min_dist=0.0, metric='cosine', low_memory=False, n_jobs=1)
            hdbscan_model = HDBSCAN(min_cluster_size=safe_min_topic_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True, core_dist_n_jobs=1)

            # ğŸ’¡ NEW FEATURE: Define the Representation Models dictionary
            # This tells BERTopic to calculate these specific variations
            representation_model = {
                "KeyBERT": KeyBERTInspired(),
                "MMR": MaximalMarginalRelevance(diversity=0.3)
            }

            topic_model = BERTopic(
                language=language,
                nr_topics=nr_topics if nr_topics == "auto" else int(nr_topics),
                min_topic_size=safe_min_topic_size,
                vectorizer_model=vectorizer_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                
                # ğŸ’¡ PASS THE MODELS HERE
                representation_model=representation_model,
                
                verbose=True
            )

            # 4. Fit
            st.write(t('step_2'))
            clean_docs = [str(d) for d in docs]
            
            try:
                topics, probs = topic_model.fit_transform(clean_docs)
            except ValueError as ve:
                st.error(f"Error: {ve}")
                st.stop()
            
            topics_list = np.array(topics).flatten().tolist()
            topics_list = [int(t) for t in topics_list]
            
            if len(clean_docs) != len(topics_list):
                min_len = min(len(clean_docs), len(topics_list))
                clean_docs = clean_docs[:min_len]
                topics_list = topics_list[:min_len]

            st.session_state['model'] = topic_model
            st.session_state['docs'] = clean_docs
            st.session_state['topics'] = topics_list
            
            st.success(t('train_complete').format(time.time() - start_time))
            status.update(label="Done", state="complete", expanded=False)

        except Exception as e:
            st.error(f"Error: {str(e)}")

# -----------------------------------------------------------------------------
# Visualization Section
# -----------------------------------------------------------------------------
if 'model' in st.session_state:
    model = st.session_state['model']
    topic_info = model.get_topic_info()
    real_topic_count = len(topic_info) - 1 
    has_topics = real_topic_count > 0
    
    st.markdown("<br><br><br>", unsafe_allow_html=True) 
    st.divider()
    st.header(t('results_header'))
    
    import plotly.express as px

    # ğŸ’¡ UPDATED: Added a 5th Tab
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "Topic Info", 
        "Distance Map", 
        "Bar Chart", 
        "Heatmap", 
        t('rep_tab_title') # New Tab Title
    ])
    
    with tab1:
        st.dataframe(topic_info, use_container_width=True)
        t_ids = topic_info['Topic'].values
        sel_t = st.selectbox("Explore Topic", t_ids)
        if sel_t is not None:
            st.write(model.get_topic(sel_t))

    with tab2:
        if not has_topics:
            st.warning(t('no_topics_warning'))
        elif real_topic_count < 4:
            st.info("Not enough topics for Distance Map.")
        else:
            try:
                fig = model.visualize_topics()
                st.plotly_chart(style_fig(fig), use_container_width=True)
            except Exception as e: st.warning(t('viz_error').format(e))

    with tab3:
        if not has_topics:
            st.warning(t('no_topics_warning'))
        else:
            try:
                fig = model.visualize_barchart(top_n_topics=10)
                st.plotly_chart(style_fig(fig), use_container_width=True)
            except Exception as e: st.warning(t('viz_error').format(e))

    with tab4:
        if not has_topics:
            st.warning(t('no_topics_warning'))
        else:
            try:
                fig = model.visualize_heatmap()
                st.plotly_chart(style_fig(fig), use_container_width=True)
            except Exception as e: st.warning(t('viz_error').format(e))

    # ğŸ’¡ NEW TAB: Representation Comparison
    with tab5:
        with st.expander(t('rep_help_title')):
            st.markdown(t('rep_help_text'))

        if has_topics and hasattr(model, 'topic_aspects_'):
            st.subheader("Comparison Table")
            
            # Helper to format keyword lists into strings
            def get_keywords_str(aspect_name, topic_id):
                if aspect_name not in model.topic_aspects_: return ""
                if topic_id not in model.topic_aspects_[aspect_name]: return ""
                # Take top 5 words
                words = [x[0] for x in model.topic_aspects_[aspect_name][topic_id][:5]]
                return ", ".join(words)

            # Build a comparison DataFrame
            # Start with basic topic info
            comp_df = topic_info[['Topic', 'Count', 'Name']].copy()
            comp_df.rename(columns={'Name': 'Default (c-TF-IDF)'}, inplace=True)
            
            # Add KeyBERT column
            comp_df['KeyBERT Inspired'] = comp_df['Topic'].apply(lambda x: get_keywords_str('KeyBERT', x))
            
            # Add MMR column
            comp_df['MMR (Diversity)'] = comp_df['Topic'].apply(lambda x: get_keywords_str('MMR', x))
            
            # Filter out -1 outlier if desired, or keep it. Let's keep it but put it at the end.
            st.dataframe(comp_df, use_container_width=True)
            
        else:
            st.info("No alternative representations found. Please retrain the model.")

elif not docs:
    st.info(t('upload_prompt'))
